{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf5d011",
   "metadata": {},
   "source": [
    "# Golden Dataset Construction (Extraction → Label Expansion → Cleaning)\n",
    "\n",
    "This notebook consolidates the **Golden Dataset** workflow used in the capstone project *System Risk in Policy-Driven AI Systems*.\n",
    "\n",
    "It includes two stages:\n",
    "1. **Extraction / sampling** from the Civil Comments dataset into a small, governance-focused subset (high-risk, borderline, clean tiers).\n",
    "2. **Label expansion & cleaning** to produce analysis-ready columns (binary label columns, ambiguity codes, and a cleaned export).\n",
    "\n",
    "> **Note:** To keep this repository ethical and lightweight, **raw datasets are not included**.  \n",
    "> You must provide the Civil Comments source file locally (or via Drive) and choose your own input/output paths below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819581c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display tweaks (optional)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "print(\"✅ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be1047",
   "metadata": {},
   "source": [
    "## 0) Configure file paths\n",
    "\n",
    "Set paths to:\n",
    "- the **Civil Comments** processed CSV used for extraction (contains toxicity label columns)\n",
    "- the **pre-clean Golden Dataset** file (if you labeled in a spreadsheet)\n",
    "- output locations (kept out of version control)\n",
    "\n",
    "Recommended: store local artifacts in a folder named `outputs/` and add it to `.gitignore`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REQUIRED: Civil Comments source for sampling/extraction ---\n",
    "CIVIL_COMMENTS_PATH = \"<PATH_TO_CIVIL_COMMENTS_READY_CSV>\"  # e.g., \"data/Civil_Comments_TFDS.csv\"\n",
    "\n",
    "# --- OPTIONAL: a pre-clean Golden Dataset file (spreadsheet) after human labeling ---\n",
    "GOLD_PRE_CLEAN_PATH = \"<PATH_TO_GOLDEN_PRECLEAN_XLSX>\"      # e.g., \"outputs/golden_dataset_preclean.xlsx\"\n",
    "GOLD_PRE_CLEAN_SHEET = \"golden_dataset\"\n",
    "\n",
    "# --- OUTPUTS (recommended to keep out of git) ---\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "EXTRACTED_OUTPUT_PATH = f\"{OUTPUT_DIR}/golden_subset_extracted.csv\"\n",
    "CLEANED_OUTPUT_PATH  = f\"{OUTPUT_DIR}/golden_dataset_clean.xlsx\"\n",
    "\n",
    "print(\"Configured paths. Update the <...> placeholders before running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9322e2c",
   "metadata": {},
   "source": [
    "## 1) Load Civil Comments source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e54253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset used for extraction\n",
    "# Expected columns: `comment_text` (or similar) and the 7 label score columns:\n",
    "# toxicity, severe_toxicity, obscene, insult, threat, identity_attack, sexual_explicit\n",
    "\n",
    "df = pd.read_csv(CIVIL_COMMENTS_PATH)\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", len(df.columns))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb94fb0",
   "metadata": {},
   "source": [
    "## 2) Golden subset extraction / sampling (risk tiers + per-label minimums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c168ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Step 10 – Golden Dataset Sampling with Per-Label Minimums (≥ 15)\n",
    "# ======================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "label_cols = [\n",
    "    'toxicity', 'severe_toxicity', 'obscene',\n",
    "    'insult', 'threat', 'identity_attack', 'sexual_explicit'\n",
    "]\n",
    "\n",
    "# Targets by risk tier: keep grand total = 313\n",
    "tier_targets = {'High-Risk': 186, 'Borderline': 62, 'Clean': 62}\n",
    "\n",
    "# Minimum presence per label (for analysis power):\n",
    "# We will count a row toward label L if L-score ≥ 0.30 (present).\n",
    "min_per_label = 15\n",
    "\n",
    "# Work on a copy to avoid side effects\n",
    "pool = tiered_df.copy()\n",
    "\n",
    "# Track selections\n",
    "selected_idx = set()\n",
    "\n",
    "# Track how many we have taken from each tier so far\n",
    "tier_counts = {'High-Risk': 0, 'Borderline': 0, 'Clean': 0}\n",
    "\n",
    "# Helper: sample rows safely from a candidate subset, honoring tier quotas and avoiding duplicates\n",
    "def take_from_pool(candidates, needed, tier_name):\n",
    "    \"\"\"Return a list of indices selected from 'candidates' up to 'needed',\n",
    "    without exceeding the remaining tier quota or duplicating rows.\"\"\"\n",
    "    remaining_quota = max(0, tier_targets[tier_name] - tier_counts[tier_name])\n",
    "    k = min(needed, remaining_quota, len(candidates))\n",
    "    if k <= 0:\n",
    "        return []\n",
    "    taken = candidates.sample(n=k, random_state=42).index.tolist()\n",
    "    return taken\n",
    "\n",
    "# 1) Ensure ≥ 15 presence per label using High-Risk first (≥0.50), then Borderline (0.30–0.49)\n",
    "for L in label_cols:\n",
    "    # Count current presence already selected (initially 0)\n",
    "    def presence_mask(df):\n",
    "        return df[L] >= 0.30  # presence definition for later per-label analysis\n",
    "\n",
    "    current_presence = 0\n",
    "\n",
    "    # High-Risk pool for this label (presence AND risk_tier == 'High-Risk')\n",
    "    hr_pool = pool[\n",
    "        (pool.index.isin(selected_idx) == False) &\n",
    "        (pool['risk_tier'] == 'High-Risk') &\n",
    "        (pool[L] >= 0.50)\n",
    "    ]\n",
    "\n",
    "    # Take as many as possible toward the min requirement from High-Risk\n",
    "    need = max(0, min_per_label - current_presence)\n",
    "    hr_take = take_from_pool(hr_pool, need, 'High-Risk')\n",
    "    selected_idx.update(hr_take)\n",
    "    tier_counts['High-Risk'] += len(hr_take)\n",
    "    current_presence += len(hr_take)\n",
    "\n",
    "    # If still short, top up from Borderline presence (0.30–0.49) where risk_tier == 'Borderline'\n",
    "    if current_presence < min_per_label:\n",
    "        bl_pool = pool[\n",
    "            (pool.index.isin(selected_idx) == False) &\n",
    "            (pool['risk_tier'] == 'Borderline') &\n",
    "            (pool[L] >= 0.30) & (pool[L] < 0.50)\n",
    "        ]\n",
    "        need = max(0, min_per_label - current_presence)\n",
    "        bl_take = take_from_pool(bl_pool, need, 'Borderline')\n",
    "        selected_idx.update(bl_take)\n",
    "        tier_counts['Borderline'] += len(bl_take)\n",
    "        current_presence += len(bl_take)\n",
    "\n",
    "    # If still short (extremely rare labels), allow spillover from remaining High-Risk presence again,\n",
    "    # even if not dominant, as long as L >= 0.30 and risk_tier == 'High-Risk' (covers cases with another label slightly higher)\n",
    "    if current_presence < min_per_label:\n",
    "        hr_any_pool = pool[\n",
    "            (pool.index.isin(selected_idx) == False) &\n",
    "            (pool['risk_tier'] == 'High-Risk') &\n",
    "            (pool[L] >= 0.30)\n",
    "        ]\n",
    "        need = max(0, min_per_label - current_presence)\n",
    "        hr_any_take = take_from_pool(hr_any_pool, need, 'High-Risk')\n",
    "        selected_idx.update(hr_any_take)\n",
    "        tier_counts['High-Risk'] += len(hr_any_take)\n",
    "        current_presence += len(hr_any_take)\n",
    "\n",
    "    # NOTE: We do NOT pull presence from Clean for per-label minima,\n",
    "    # because presence ≥ 0.30 by definition belongs to High-Risk/Borderline thresholds.\n",
    "\n",
    "# 2) After guaranteeing per-label minima, fill remaining quotas per tier at random (stratified only by tier).\n",
    "#    This preserves organic composition while completing 186 / 62 / 62.\n",
    "for tier_name in ['High-Risk', 'Borderline', 'Clean']:\n",
    "    remaining = max(0, tier_targets[tier_name] - tier_counts[tier_name])\n",
    "    if remaining == 0:\n",
    "        continue\n",
    "\n",
    "    tier_pool = pool[\n",
    "        (pool.index.isin(selected_idx) == False) &\n",
    "        (pool['risk_tier'] == tier_name)\n",
    "    ]\n",
    "\n",
    "    # If the tier has fewer rows than needed, take all; otherwise sample\n",
    "    if len(tier_pool) <= remaining:\n",
    "        take_idx = tier_pool.index.tolist()\n",
    "    else:\n",
    "        take_idx = tier_pool.sample(n=remaining, random_state=42).index.tolist()\n",
    "\n",
    "    selected_idx.update(take_idx)\n",
    "    tier_counts[tier_name] += len(take_idx)\n",
    "\n",
    "# 3) Build the final DataFrame, shuffle, and double-check totals\n",
    "golden_df = pool.loc[list(selected_idx)].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Safety trims if overshoot (should not happen, but keep consistent with prior pattern)\n",
    "if len(golden_df) > 313:\n",
    "    golden_df = golden_df.sample(n=313, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 4) Quick verification\n",
    "print(\"✅ Golden Dataset (313 samples) created with per-label presence ≥ 15 and exact tier totals.\\n\")\n",
    "print(\"Tier counts:\\n\", golden_df['risk_tier'].value_counts().to_string(), \"\\n\")\n",
    "\n",
    "# Per-label PRESENCE counts (rows where label score ≥ 0.30 anywhere in the row)\n",
    "presence_summary = {}\n",
    "for L in label_cols:\n",
    "    presence_summary[L] = int((golden_df[L] >= 0.30).sum())\n",
    "print(\"Per-label presence (≥ 0.30) counts:\\n\", pd.Series(presence_summary).to_string(), \"\\n\")\n",
    "\n",
    "# Optional: show dominant_label cross-tab for context (not the enforcement metric)\n",
    "print(\"Dominant label distribution (context only):\\n\")\n",
    "print(golden_df['dominant_label'].value_counts().to_string())\n",
    "\n",
    "# 5) Save final dataset\n",
    "output_path = \"<SET_OUTPUT_PATH>\"\n",
    "golden_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb40e5b",
   "metadata": {},
   "source": [
    "## 3) Load pre-clean Golden Dataset (after human labeling)\n",
    "\n",
    "If you labeled the extracted subset in a spreadsheet (Excel/Google Sheets export), load it here.\n",
    "This stage:\n",
    "- parses label list columns (e.g., `first_label`, `final_label`)\n",
    "- expands them into binary indicator columns\n",
    "- converts ambiguity text into numeric codes\n",
    "- drops helper/duplicate columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled spreadsheet (pre-clean)\n",
    "# Make sure the sheet contains columns like: first_label, final_label, ambiguity, final_ambiguity\n",
    "df_gold = pd.read_excel(GOLD_PRE_CLEAN_PATH, sheet_name=GOLD_PRE_CLEAN_SHEET)\n",
    "\n",
    "print(\"Rows:\", len(df_gold))\n",
    "df_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: ensure list-like label columns ---\n",
    "# If labels are stored as strings like \"['toxicity','insult']\" convert them to Python lists.\n",
    "\n",
    "import ast\n",
    "\n",
    "def to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            # fallback: split on commas if needed\n",
    "            return [t.strip() for t in x.strip(\"[]\").replace(\"'\", \"\").split(\",\") if t.strip()]\n",
    "    return []\n",
    "\n",
    "for col in [\"first_label\", \"final_label\"]:\n",
    "    if col in df_gold.columns:\n",
    "        df_gold[col] = df_gold[col].apply(to_list)\n",
    "\n",
    "df_gold[[\"first_label\",\"final_label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4de63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand label lists into binary columns\n",
    "ALL_LABELS = [\n",
    "    \"toxicity\", \"severe_toxicity\", \"obscene\",\n",
    "    \"insult\", \"threat\", \"identity_attack\", \"sexual_explicit\"\n",
    "]\n",
    "\n",
    "for label in ALL_LABELS:\n",
    "    df_gold[f\"first_{label}\"] = df_gold[\"first_label\"].apply(lambda lst: 1 if label in lst else 0)\n",
    "    df_gold[f\"final_{label}\"] = df_gold[\"final_label\"].apply(lambda lst: 1 if label in lst else 0)\n",
    "\n",
    "df_gold[[c for c in df_gold.columns if c.startswith(\"first_\") or c.startswith(\"final_\")]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ambiguity text into numeric codes\n",
    "ambiguity_map = {\n",
    "    \"no_violation\": 0,\n",
    "    \"gray_area\": 1,\n",
    "    \"clear_violation\": 2\n",
    "}\n",
    "\n",
    "if \"ambiguity\" in df_gold.columns:\n",
    "    df_gold[\"ambiguity_code\"] = df_gold[\"ambiguity\"].map(ambiguity_map)\n",
    "\n",
    "if \"final_ambiguity\" in df_gold.columns:\n",
    "    df_gold[\"final_ambiguity_code\"] = df_gold[\"final_ambiguity\"].map(ambiguity_map)\n",
    "\n",
    "df_gold[[c for c in [\"ambiguity\",\"final_ambiguity\",\"ambiguity_code\",\"final_ambiguity_code\"] if c in df_gold.columns]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: drop columns you don't want in the final clean export\n",
    "COLUMNS_TO_DROP = [\n",
    "    # add any helper columns here if needed\n",
    "]\n",
    "\n",
    "df_gold_clean = df_gold.drop(columns=COLUMNS_TO_DROP, errors=\"ignore\").copy()\n",
    "\n",
    "print(\"Clean columns:\", len(df_gold_clean.columns))\n",
    "df_gold_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned Golden Dataset\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "df_gold_clean.to_excel(CLEANED_OUTPUT_PATH, index=False)\n",
    "print(\"✅ Saved:\", CLEANED_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
